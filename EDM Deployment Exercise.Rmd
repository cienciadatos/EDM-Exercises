---
title: "EDM - Deployment Exercise"
author: "Pablo Bolta, Jacinto Dobón y Jorge López"
date: "5/4/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(plotly)
library(reshape2)
library(lubridate)
library(randomForestSRC)
```

## 1.- One dimensional Partial Dependence Plot.
The partial dependence plot shows the marginal effect of a feature on the predicted outcome of a previously fit model. 
 
### EXERCISE:
Apply PDP to the regression example of predicting bike rentals. Fit a random forest approximation for the prediction of bike rentals (cnt). Use the partial dependence plot to visualize the relationships the model learned. Use the slides shown in class as model.  

### QUESTION:
Analyse the influence of days since 2011, temperature, humidity and wind speed on the predicted bike counts.

___________

First, we load the data from the data folder, obtained by using DVC. It contains the .csv and .dvc files, stored in the Google Drive repository. Once the data is loaded, we proceed to transform the variables and filter by date from 2011 onwards. 

```{r exercise_1}
days <- read.csv("data/day.csv")

days$dteday <- as_date(days$dteday)
days_since <- select(days, workingday, holiday, temp, hum, windspeed, cnt)
days_since$days_since_2011 <- int_length(interval(ymd("2011-01-01"), days$dteday)) / (3600*24)
days_since$SUMMER <- ifelse(days$season == 3, 1, 0)
days_since$FALL <- ifelse(days$season == 4, 1, 0)
days_since$WINTER <- ifelse(days$season == 1, 1, 0)
days_since$MISTY <- ifelse(days$weathersit == 2, 1, 0)
days_since$RAIN <- ifelse(days$weathersit == 3 | days$weathersit == 4, 1, 0)
days_since$temp <- days_since$temp * 47 - 8
days_since$hum <- days_since$hum * 100
days_since$windspeed <- days_since$windspeed * 67
```

Once we finished the transformation and filtering of the variables, we proceeded to the training of the model with Random Forest. In the same way, we perform the corresponding predictions. The code is shown below:

```{r rf_model_and_results}
model.rf <- rfsrc(cnt~., data=days_since)

results <- select(days_since, days_since_2011, temp, hum, windspeed, cnt)
len <- nrow(days_since)
for(c in names(results)[1:4])
{
  for(i in 1:len){
    r <- days_since
    r[[c]] <- days_since[[c]][i]
    pred <- predict(model.rf, r)$predicted
    results[[c]][i] <- sum(pred) / len
  }
}
```

Finally, we plotted the results using the partial dependence plot for the "days_since_2011", "temp", "hum" and "windspeed" variables.

```{r plot_results}
plot_1 <- ggplot(days_since, aes(x=days_since_2011, y=results$days_since_2011)) +
          geom_line() + 
          ylim(c(0,6000)) + 
          geom_rug(alpha=0.1, sides="b") + 
          ylab("Prediction") + 
          xlab("Days since 2011")

plot_2 <- ggplot(days_since, aes(x=temp, y=results$temp)) + 
          geom_line() + 
          ylim(c(0,6000)) + 
          geom_rug(alpha=0.1, sides="b") + 
          xlab("Temperature")

plot_3 <- ggplot(days_since, aes(x=hum, y=results$hum)) + 
          geom_line() + 
          ylim(c(0,6000)) + 
          geom_rug(alpha=0.1, sides="b") + 
          xlab("Humidity")

plot_4 <- ggplot(days_since, aes(x=windspeed, y=results$windspeed)) + 
          geom_line() + 
          ylim(c(0,6000)) + 
          geom_rug(alpha=0.1, sides="b") + 
          xlab("Wind speed")

subplot(plot_1, plot_2, plot_3, plot_4, 
          shareX = FALSE, 
          shareY = TRUE, 
          titleX = TRUE)
```

### INTERPRETATION

**`Days since 2011`**: As we move forward in time (since 2011), bicycle rentals tend to increase. It is important to highlight two fundamental aspects:

   1. Between 130 and 350 days, the difference in importance is minimal in this variable (always around 3700-3800 more bicycles).
   2. The trend is always increasing, except in the final stretch. From day 648 onwards, the number of bicycles rented decreases from 5679 to 5104. 
   
These explanations are reliable, since we have observations for all values of this variable.

**`Temperature`**: The most remarkable aspect of this variable is that as the temperature increases, more bicycles are rented, from 3180 bicycles at -5 degrees to 5274 bicycles at 20 degrees. In general, more bicycles are rented when the temperature is pleasant (16-26 degrees). When it is cold or hot, fewer and fewer bicycles are rented.

**`Humidity`**: For humidity values below 50%, the number of bicycles rented remains constant (about 4700 bicycles). On the other hand, as humidity increases (from 50%), fewer and fewer bicycles are rented, reaching its minimum (3704 bicycles) when humidity reaches 97%. However, these explanations should be taken with caution when humidity is below 37% or above 92%, as in those cases there are not many observations, making the explanations not entirely reliable.

**`Wind speed`**: The model predicts that the trend of this variable is clearly decreasing. As wind speed increases, fewer bicycles are rented, going from 4636 with wind speed equal to 1.5; to 4178 when wind speed has a value of 24. For values greater than 24, the number of bicycles rented remains stable at 4178. However, for values greater than 24, the reliability of these explanations must be reconsidered, as there are very few observations.

## 2.- Bidimensional Partial Dependency Plot.

### EXERCISE:
Generate a 2D Partial Dependency Plot with humidity and temperature to predict the number of bikes rented depending on those parameters.

> **BE CAREFUL: due to the size, extract a set of random samples from the BBDD before generating the data for the Partial Dependency Plot.**

Show the density distribution of both input features with the 2D plot as shown in the class slides. 

TIP: Use geom_tile() to generate the 2D plot. Set width and height to avoid holes. 

### QUESTION:
Interpret the results.
___________

We will use the previously trained random forest model to make predictions on temperature and humidity data. Due to the volume of data we will randomly extract 50 observations. And we will complete the dataset in order to represent the 2D Partial Dependency Plot.

```{r 2d_pdp}
# Set the number of rows in the dataset
nr <- nrow(days_since)

# Sample 40 observations from the dataset
sampled <- sample_n(days_since, 50)

# Extract the temperature and humidity values from the sampled dataset
temp <- sampled$temp
hum <- sampled$hum

# Combine the temperature and humidity values into a single dataframe
th <- merge(data.frame(temp), data.frame(hum), by=NULL)
# Remove duplicates from the dataframe
th <- unique(th)

# Add a new column to store the predictions
th$predicted <- 0.0

# Loop through each row in the temperature/humidity dataframe
for(i in 1:nrow(th)){
  
  # Copy the original dataset to a new variable
  r <- days_since
  
  # Set the temperature and humidity values in the new dataset to the current row's values
  r[["temp"]] <- th[["temp"]][i]
  r[["hum"]] <- th[["hum"]][i]
  
  # Make a prediction using the random forest model
  sal <- predict(model.rf, r)$predicted
  
  # Calculate the average of all the predictions
  th[["predicted"]][i] <- sum(sal) / nr
}
```
We plot the results using the two-dimensional partial dependence plot for the variables "temp" and "hum".

```{r 2d_pdp_plot}
# Create a heatmap using ggplot2
ggplot(th, aes(x=temp, y=hum)) + 
  # Use the predicted values to fill the tiles
  geom_tile(aes(fill=predicted, width=10, height=15)) + 
  # Add a rug plot to show the distribution of the sampled points
  geom_rug(alpha=0.01) +
  # Add labels to the x and y axes
  labs(x = "Temperature (°C)", y = "Humidity (%)") +
  # Add a title to the colorbar
  scale_fill_continuous(name = "ypred")
```
Finally, we also include the one-dimensional plot of the extracted samples to ensure that the shape of the data is maintained.
```{r sampled_1d_pdp}

results2 <- select(sampled, temp, hum, cnt)
len <- nrow(sampled)
for(c in names(results2)[1:2])
{
  for(i in 1:len){
    r <- sampled
    r[[c]] <- sampled[[c]][i]
    pred <- predict(model.rf, r)$predicted
    results2[[c]][i] <- sum(pred) / len
  }
}


plot_22 <- ggplot(sampled, aes(x=temp, y=results2$temp)) + 
          geom_line() + 
          ylim(c(0,6000)) + 
          geom_rug(alpha=0.1, sides="b") + 
          xlab("Temperature (ºC)") + 
          ylab("Predicted number of bike rentals")

plot_23 <- ggplot(sampled, aes(x=hum, y=results2$hum)) + 
          geom_line() + 
          ylim(c(0,6000)) + 
          geom_rug(alpha=0.1, sides="b") + 
          xlab("Humidity (%)")

subplot(plot_22, plot_23,  
          shareX = FALSE, 
          shareY = TRUE, 
          titleX = TRUE)
  
```

## INTERPRETATION
In the 2D PDP, we observe the same phenomena as in the PDP of temperature and humidity. The maximum number of bike rentals occurs at a temperature of around 20 degrees Celsius and a humidity below 50%. On the other hand, the minimum number of bike rentals occurs when the temperature is extremely low and the humidity is very high. This confirms what we saw in the PDPs, where we observed a certain independence between the effect of temperature and humidity.

While the humidity is below 50%, the maximum number of bike rentals is achieved. However, the number of rentals decreases as the humidity increases beyond this threshold. In contrast, starting from the minimum temperature, an increase in temperature leads to an increase in bike rentals until it reaches a maximum at around 17.5 degrees Celsius. Beyond this point, the number of bike rentals remains constant until the temperature reaches 25 degrees Celsius, after which it decreases again.

Based on the observations from the 1-dimensional PDPs and the 2D PDP The effects of temperature and humidity are independent. However, it is important to note that the model has not been trained on certain scenarios where there are not many real observations, and therefore, the explanations may not be reliable. It is also important to know the number of observations for each possible interaction of the variables in the dataset. Therefore, to make more accurate predictions, it may be necessary to use or collect more data, or refine the model to include more scenarios.

## 3.- PDP to explain the price of a house.

### EXERCISE:
Apply the previous concepts to predict the price of a house from the database kc_house_data.csv. In this case, use again a random forest approximation for the prediction based on the features bedrooms, bathrooms, sqft_living, sqft_lot, floors and yr_built.Use the partial dependence plot to visualize the relationships the model learned.

> **BE CAREFUL: due to the size, extract a set of random samples from the BBDD before generating the data for the Partial Dependency Plot.** 

### QUESTION:
Analyse the influence of bedrooms, bathrooms, sqft_living and floors on the predicted price.

___________
First we load the data from the dataset, extracting 1000 random samples to reduce the size. Next we obtain the dataframe that we will use for the representation:

```{r pdp_house}
h <- read.csv("data/kc_house_data.csv")

set.seed(100)

sampled3 <- sample_n(h, 1000)
sampled3 <- select(sampled3, bedrooms, bathrooms, sqft_living, sqft_lot, floors, yr_built, price)

model3 <- rfsrc(price~., data=sampled3)

results3 <- select(sampled3, bedrooms, bathrooms, sqft_living, floors, price)
nr3 <- nrow(sampled3)
for(c in names(results3)[1:4])
{
  for(i in 1:nr3){
    r <- sampled3
    r[[c]] <- sampled3[[c]][i]
    sal <- predict(model3, r)$predicted
    results3[[c]][i] <- sum(sal) / nr3
  }
}
```

We plot the PDPs of the proposed variables as we did in exercise 1, but with a sampled version to consume less system resources:
```{r}
plot_31 <- ggplot(sampled3, aes(x=bedrooms, y=results3$bedrooms)) + geom_line() + ylim(c(300000,2000000)) + geom_rug(alpha=0.1, sides="b") + ylab("Prediction") + xlab("Bedrooms") + ylab("Predicted house price")

plot_32 <- ggplot(sampled3, aes(x=bathrooms, y=results3$bathrooms)) + geom_line() + ylim(c(300000,2000000)) + geom_rug(alpha=0.1, sides="b") + xlab("Bathrooms")

plot_33 <- ggplot(sampled3, aes(x=sqft_living, y=results3$sqft_living)) + geom_line() + ylim(c(300000,2000000)) + geom_rug(alpha=0.1, sides="b") + xlab("Sqft Living")

plot_34 <- ggplot(sampled3, aes(x=floors, y=results3$floors)) + geom_line() + ylim(c(300000,2000000)) + geom_rug(alpha=0.1, sides="b")+ xlab("Floors")

subplot(plot_31, plot_32, plot_33, plot_34, titleX = TRUE, shareX = FALSE, titleY = TRUE, shareY = TRUE)
```

## INTERPRETATION

**'bedrooms'**: This variable behaves in an interesting way, starting from 1 bedroom, the more bedrooms, the cheaper the house, up to 3 bedrooms. From this point, the trend reverses, and the more bedrooms, the higher the price. This is probably because 3 bedrooms is the most common option. Although it cannot be stated with total certainty, since there are few observations for the variable when there are more than 6 bedrooms.

**'bathrooms'**: It is clearly observed that the more bathrooms, the higher the housing price. However, this statement is not entirely reliable in the case of the outliers 0.75 and 1.25, or for more than 4 bathrooms, since there are hardly any training samples with these values.

**'Sqft_living'**: We clearly see that the larger the living area, the more expensive the housing. However, this statement may not be applicable to values below 52 square meters and above 450 square meters, which if they occur in the full data set are probably outliers. The local minimum around 33 square meters is striking, which again could be due to the fact that it is frequent in the housing stock.

**'Floors'**: In this variable, we can assert that the higher the number of floors, the higher the price of the property, and this is a reliable explanation, as there are observations for all values in the training set.
